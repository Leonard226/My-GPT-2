{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3637c777-ef9f-4e31-a5ea-325595e86665",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d50a71-0e93-419a-8f45-1ef7ccce22d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import load_encoder_hparams_and_params\n",
    "import fire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4a52a-d009-4562-a6d9-060f998ccf4d",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The class `encoder` is the BPE (Byte Pair Encoding) tokenizer used by GPT2. The `encoder.json` file consists of a long list with format`<token_ID>, \"<token>\"` and maps words/subwords to token IDs, for example: \n",
    "> `605, \"\\u0120them\": 606, \"\\u0120her\": 607, \"ount\": 608, \"\\u0120Ch\": ...`\n",
    "\n",
    "Note, `\\u0120` represents the white space character: \n",
    "- `\"\\u0120them\"` $\\to$ `\" them\"`\n",
    "- `\"\\u0120her\"` $\\to$ `\" her\"`\n",
    "\n",
    "`encoder.py` implements the translation from text prompt to token IDs using this mapping. \n",
    "\n",
    "```python\n",
    ">>> ids = encoder.encode(\"Not all heroes wear capes.\")\n",
    ">>> ids\n",
    "[3673, 477, 10281, 5806, 1451, 274, 13]\n",
    "\n",
    ">>> encoder.decode(ids)\n",
    "\"Not all heroes wear capes.\"\n",
    "```\n",
    "\n",
    "`encoder.decoder` holds the vocabulary and is of size \n",
    "```python\n",
    ">>> len(encoder.decoder)\n",
    "50257\n",
    "```\n",
    "\n",
    "Training of the tokenizer refers to how strings are broken down. When we load the tokenizer, we are loading the already trained vocab (`encoder.json`) and byte-pair merges (`vocab.bpe`) (byte-pair merges). The byte-pair merges are used to form tokens from text prompts: \n",
    "```\n",
    "('l', 'o') → 'lo'\n",
    "('o', 'w') → 'low'\n",
    "('e', 'r') → 'er'\n",
    "('e', 's') → 'es'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7ea77-6b5e-4eb3-9091-b780f002f5d3",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "`hparams.json` is a directory that contains the hyper-paramters for our model: \n",
    "```\n",
    "{\n",
    "  \"n_vocab\": 50257, # number of tokens in our vocabulary\n",
    "  \"n_ctx\": 1024,    # maximum possible sequence length of the input \n",
    "  \"n_embd\": 768,    # dimension of embddings\n",
    "  \"n_head\": 12,     # number of attention heads\n",
    "  \"n_layer\": 12     # number of layers (depth)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020c535-e226-40ce-a83f-3a983a9927c8",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "`params` holds the trained weights of our model. If we print `params`, **REPLACING** the weight arrays with their shapes, we get: \n",
    "```\n",
    "{\n",
    "    \"wpe\": [1024, 768], # positional encoding matrix for 1024 positions \n",
    "    \"wte\": [50257, 768], # embedding matrix for 50257 tokens each mappted to 768d vector\n",
    "    \"ln_f\": {\"b\": [768], \"g\": [768]}, # bias and sacling factors for LayerNorm \n",
    "    \"blocks\": [\n",
    "        {\n",
    "            \"attn\": {\n",
    "                \"c_attn\": {\"b\": [2304], \"w\": [768, 2304]}, # query, key, value matrices of size 768x2304\n",
    "                \"c_proj\": {\"b\": [768], \"w\": [768, 768]}, # projection matrix after attention \n",
    "            },\n",
    "            \"ln_1\": {\"b\": [768], \"g\": [768]}, # LayerNorm\n",
    "            \"ln_2\": {\"b\": [768], \"g\": [768]},\n",
    "            \"mlp\": {\n",
    "                \"c_fc\": {\"b\": [3072], \"w\": [768, 3072]}, # weights and bias for FFN\n",
    "                \"c_proj\": {\"b\": [768], \"w\": [3072, 768]}, # maps back to 768d\n",
    "            },\n",
    "        },\n",
    "        ... # repeat for n_layers\n",
    "    ]\n",
    "}\n",
    "```\n",
    "For reference, here are the shapes of `params` but with the numbers replaced by `hparams`:\n",
    "```\n",
    "{\n",
    "    \"wpe\": [n_ctx, n_embd],\n",
    "    \"wte\": [n_vocab, n_embd],\n",
    "    \"ln_f\": {\"b\": [n_embd], \"g\": [n_embd]},\n",
    "    \"blocks\": [\n",
    "        {\n",
    "            \"attn\": {\n",
    "                \"c_attn\": {\"b\": [3*n_embd], \"w\": [n_embd, 3*n_embd]},\n",
    "                \"c_proj\": {\"b\": [n_embd], \"w\": [n_embd, n_embd]},\n",
    "            },\n",
    "            \"ln_1\": {\"b\": [n_embd], \"g\": [n_embd]},\n",
    "            \"ln_2\": {\"b\": [n_embd], \"g\": [n_embd]},\n",
    "            \"mlp\": {\n",
    "                \"c_fc\": {\"b\": [4*n_embd], \"w\": [n_embd, 4*n_embd]},\n",
    "                \"c_proj\": {\"b\": [n_embd], \"w\": [4*n_embd, n_embd]},\n",
    "            },\n",
    "        },\n",
    "        ... # repeat for n_layers\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5d62d5-7f5a-4bbf-868f-e2675d551760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder, hparams, params = load_encoder_hparams_and_params(\"124M\", \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33758db-0b61-423f-a9c6-f39e414f86d1",
   "metadata": {},
   "source": [
    "### GELU\n",
    "The non-linearity (activation function) for GPTw is GELU (Gaussian Error Linear Units). $$\\text{GELU}(x) \\approx 0.5 x \\left( 1 + \\tanh \\left( \\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715 x^3 \\right) \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eba25c5-67a3-476b-8d66-f61919311fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f727b2-244d-4b7e-893c-60ee64a385c6",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "$$\n",
    "\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "Given a two dimensional array, we apply softmax row-wise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81349f6-b76b-44b9-91fd-59f09201c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de4edd-2da0-452b-bb8f-e13bc04292a4",
   "metadata": {},
   "source": [
    "### Layer Normilization\n",
    "Normalizes values to have mean $\\mu=0$ and variance $\\sigma^2=1$. It is meant to stabilize training by ensuring that inputs to each layer have a consistent distribution. \n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2}} + \\beta\n",
    "$$\n",
    "Where $\\gamma$ (scale) and $\\beta$ (shift) are learnable weights. Note that we first normalize to zero mean and unit variance, but then we use the learned weights to allow the network to undo or adjust the effect if needed. So we give the network the flexibility to decide. \n",
    "\n",
    "Given a two dimensional arras, we apply normalization row-wise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad826295-d8fd-423f-add1-76ec07fee1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, g, b, eps: float = 1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
    "    return g * x + b  # scale and offset with gamma/beta params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2197fb3-1cd1-4673-802e-5e2444e42025",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "Feed the embeddings independently into the linear layer. The bias `b` is added independently to each row. \n",
    "$$\n",
    "\\text{Linear}(X) = XW + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24c2f12-b950-4f7f-8f2d-c0c876aa5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(X, w, b): \n",
    "    return X @ w + b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc77fd5-6f9e-47b9-8adb-5cf56e72c935",
   "metadata": {},
   "source": [
    "### GPT Architecture\n",
    "At a high level, the GPT architecture has three sections: \n",
    "- Text + Positional Embeddings\n",
    "- Transformer Decoder Stack\n",
    "- Projection to Vocab\n",
    "\n",
    "The `gpt2` function is the actual GPT code and implements its architecture and forward pass. It gets called by `generate()` upon each token that has to be generated given the previous generated token IDs. \n",
    "Parameter List:\n",
    "- `inputs`: token IDs of previously generated input or user text prompt\n",
    "- `wte`: weights embedding matrix\n",
    "- `wpe`: weights positional encoding\n",
    "- `blocks`: number of blocks\n",
    "- `ln_f`: layerNorm \n",
    "- `n_heads`number of attention heads\n",
    "\n",
    "Output: \n",
    "- outputs `logits`, i.e. the probability distribution over possible next tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d1cdbe-be52-4df8-b66a-0f5f5fc44935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
    "    \n",
    "    #### TEXT + POSITIONAL ENCODING:\n",
    "    # select corresponding rows from embedding matrix \n",
    "    # when input is of size n_seq, then select the first n_seq rows of the PE Mat as the encodings are fixed and don't depend on token values      \n",
    "\n",
    "    X = wte[inputs] + wpe[:len(inputs)] # n_seq x n_embd\n",
    "\n",
    "    # X[i] represents embedding for the i-th row + positional encoding for i-th position\n",
    "    \n",
    "    ### TRANSFORMER DECODER STACK:\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks: \n",
    "        X = transformer_block(X, **block, n_head=n_head) # n_seq x n_embd -> n_seq x n_embd\n",
    "\n",
    "\n",
    "    ### PROJECTION TO VOCAB:\n",
    "    # reuse embedding matrix for projections (other implementations choose separate matrix) \n",
    "    # dont apply softmax in the end, so outputs are logits \n",
    "    X = layer_norm(X, **ln_f) # n_seq x n_embd -> n_seq x n_embd\n",
    "    return X @ wte.T # n_seq x n_embd -> n_seq x n_vocab \n",
    "\n",
    "\n",
    "def transformer_block(X, mlp, attn, ln_1, ln_2, n_head): \n",
    "    \n",
    "    ### MULTI-HEAD ATTENTION:\n",
    "    # with residual connection\n",
    "    X = X + mha(layer_norm(X, **ln_1), **attn, n_head=n_head)\n",
    "    \n",
    "    ### FEED FORWARD NETWORK:\n",
    "    # with residual connection\n",
    "    X = X + ffn(layer_norm(X, **ln_2), **mlp)\n",
    "    return X\n",
    "\n",
    "\n",
    "def ffn(X, c_fc, c_proj):\n",
    "    # project from n_embd to higher dimension 4*n_embd and then back \n",
    "    X = linear(X, **c_fc) \n",
    "    Z = gelu(X)\n",
    "    X = linear(Z, **c_proj)\n",
    "    return X\n",
    "\n",
    "\n",
    "def mha(X, c_attn, c_proj, n_head): \n",
    "    X = linear(X, **c_attn)\n",
    "    qkv = np.split(X, 3, axis=-1)\n",
    "\n",
    "    # split into heads\n",
    "    qkv_heads = list(map(lambda X: np.split(X, n_head, axis=-1), qkv))\n",
    "    \n",
    "    # causual mask     \n",
    "    mask = (1 - np.tri(X.shape[0], dtype=X.dtype)) * -1e10   \n",
    "    \n",
    "    # perform attention over each head (usually done in parallel)\n",
    "    out_heads = [attention(q, k, v, mask) for q, k, v in zip(*qkv_heads)] \n",
    "\n",
    "    # concatenate heads\n",
    "    X = np.hstack(out_heads)\n",
    "\n",
    "    # final projection\n",
    "    X = linear(X, **c_proj)\n",
    "\n",
    "    return X \n",
    "    \n",
    "    \n",
    "\n",
    "def attention(q, k, v, mask): \n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381105e4-b4b0-4939-a106-b8c109d98e61",
   "metadata": {},
   "source": [
    "The `generate()` function is the autoregressive decoding altorihm we saw earlier. We use greedy sampling for simplicity. `tqdm` is a progress bar to help us visualize the decoding process as it generates tokens on at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3120f92-5e2f-491a-bb3e-cee28d17957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "\n",
    "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
    "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
    "        inputs.append(int(next_id))  # append prediction to input\n",
    "\n",
    "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa007d-331e-4658-9b54-803a092ee7ec",
   "metadata": {},
   "source": [
    "The `main` function handles: \n",
    "1. Loading the tokenizer (`encoder`), model weights (`params`), and hyperparameters (`hparams`)\n",
    "2. Encoding the input prompt into token IDs using the tokenizer\n",
    "3. Calling the generate function\n",
    "4. Decoding the output IDs into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f6f863f-689d-49a1-aabb-9bec37fa4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(prompt: str, n_tokens_to_generate: int = 40, model_size: str = \"124M\", models_dir: str = \"models\"):\n",
    "\n",
    "    # load encoder, hparams, and params from the released open-ai gpt-2 files -> done\n",
    "    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
    "\n",
    "    # encode the input string using the BPE tokenizer\n",
    "    input_ids = encoder.encode(prompt)\n",
    "\n",
    "    # make sure we are not surpassing the max sequence length of our model\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "\n",
    "    \n",
    "    # generate output ids\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "\n",
    "    # decode the ids back into a string\n",
    "    output_text = encoder.decode(output_ids)\n",
    "\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53dff53-f076-434e-bb8d-035b213f7ef1",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Here we provide the input text prompt to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d591022-910a-4ce5-bb31-de7c95b0c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|███████████████████████████████| 11/11 [00:12<00:00,  1.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' his ability to play the ball and to create chances.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(\"Christiano Ronaldo is very famous for\", n_tokens_to_generate=11, model_size=\"124M\", models_dir=\"models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
